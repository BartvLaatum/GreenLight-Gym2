hyperparameters:
  total_timesteps: 5_000_000
  n_envs: 8
  policy: MlpPolicy
  learning_rate: 7.e-5
  buffer_size: 1_000_000  
  learning_starts: 50_000 # let the agents randomly explore for episodes.
  batch_size: 1024
  tau: 0.005
  gamma: 0.99
  ent_coef: auto
  train_freq: 1
  gradient_steps: 1
  action_noise: 
    normalactionnoise:
      sigma: 0.05
  replay_buffer_class: null
  replay_buffer_kwargs: null
  optimize_memory_usage: False
  target_update_interval: 1
  use_sde: False
  sde_sample_freq: -1
  use_sde_at_warmup: False
  stats_window_size: 100

  policy_kwargs: {net_arch: {pi: [256, 256], vf: [512, 512]},
                  optimizer_class: adam,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: tanh,
                  log_std_init: np.log(1) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5, where np.log(1) results in std of 1
          }

# stats_window_size=100, tensorboard_log=None, policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True